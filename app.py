# app.py
import os
import time
import random
from pathlib import Path
from typing import Iterable, List, Dict, Tuple

import pandas as pd
import streamlit as st
import docx
from PyPDF2 import PdfReader

import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

from prompt import PROMPT_FTE  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå prompt.py ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® PROMPT_FTE

# ====== NEW: retrieval deps (TF-IDF) ======
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# =========================
# PATHS & DISCOVERY
# =========================
BASE_DIR = Path(__file__).resolve().parent

DOC_EXTS     = {".docx"}
TABULAR_EXTS = {".csv", ".xlsx", ".xls"}
PDF_EXTS     = {".pdf"}

AVATAR_PATH = BASE_DIR / "assets" / "green-bot.png"
PAGE_ICON = str(AVATAR_PATH) if AVATAR_PATH.exists() else None

# =========================
# PAGE CONFIG & HEADER
# =========================
st.set_page_config(page_title="FTE Chatbot ‚Ä¢ KMUTNB", page_icon=PAGE_ICON, layout="centered")
st.title("üó®Ô∏è FTE Chatbot ‚Ä¢ KMUTNB")

# =========================
# API KEY & MODEL CONFIG
# =========================
api_key = st.secrets.get("GEMINI_APIKEY")
if not api_key:
    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_APIKEY ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .streamlit/secrets.toml ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤.")
    st.stop()

genai.configure(api_key=api_key)

generation_config = {
    "temperature": 0.1,
    "top_p": 0.95,
    "top_k": 64,
    "max_output_tokens": 1024,
    "response_mime_type": "text/plain",
}
SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}
PRIMARY_MODEL_NAME = "gemini-2.0-flash"
FALLBACK_MODEL_NAME = "gemini-1.5-flash"

def make_model(name: str) -> genai.GenerativeModel:
    return genai.GenerativeModel(
        model_name=name,
        safety_settings=SAFETY_SETTINGS,
        generation_config=generation_config,
        system_instruction=PROMPT_FTE,
    )

# =========================
# CHAT HISTORY UTILS
# =========================
def clear_history():
    st.session_state["previous_messages"] = st.session_state.get("messages", []).copy()
    st.session_state["messages"] = [{"role": "assistant", "content": "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡πÄ‡∏ä‡∏ó‡∏Ç‡∏≠‡∏á‡∏ó‡πà‡∏≤‡∏ô"}]
    st.rerun()

def restore_history():
    if st.session_state.get("previous_messages"):
        st.session_state["messages"] = st.session_state["previous_messages"].copy()
    else:
        st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏Ñ‡∏∑‡∏ô‡πÑ‡∏î‡πâ")
    st.rerun()

# =========================
# FILE READERS (CACHED)
# =========================
@st.cache_data(show_spinner=False)
def extract_text_from_docx(docx_path: str) -> str:
    try:
        d = docx.Document(docx_path)
        return "\n".join([p.text for p in d.paragraphs if p.text.strip()])
    except Exception as e:
        st.error(f"Error reading Word file '{docx_path}': {e}")
        return ""

@st.cache_data(show_spinner=False)
def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        reader = PdfReader(pdf_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() or ""
        return text
    except Exception as e:
        st.error(f"Error reading PDF file '{pdf_path}': {e}")
        return ""

@st.cache_data(show_spinner=False)
def load_excel_as_text(excel_path: str, max_rows: int = 160, max_cols: int = 12) -> str:
    try:
        if not os.path.exists(excel_path):
            return ""
        df = pd.read_excel(excel_path)
        if df.shape[1] > max_cols:
            df = df.iloc[:, :max_cols]
        if len(df) > max_rows:
            df = df.head(max_rows)
        return df.to_csv(index=False)
    except Exception as e:
        st.error(f"Error reading Excel file '{excel_path}': {e}")
        return ""

@st.cache_data(show_spinner=False)
def load_csv_as_text(csv_path: str, max_rows: int = 200, max_cols: int = 12) -> str:
    try:
        if not os.path.exists(csv_path):
            return ""
        df = pd.read_csv(csv_path, engine="python", encoding_errors="ignore")
        if df.shape[1] > max_cols:
            df = df.iloc[:, :max_cols]
        if len(df) > max_rows:
            df = df.head(max_rows)
        return df.to_csv(index=False)
    except Exception as e:
        st.error(f"Error reading CSV file '{csv_path}': {e}")
        return ""

# =========================
# DISCOVER FILES (RECURSIVE)
# =========================
def rglob_many(root: Path, exts: Iterable[str]) -> list[Path]:
    exts_low = {e.lower() for e in exts}
    return [p for p in root.rglob("*") if p.is_file() and p.suffix.lower() in exts_low]

@st.cache_data(show_spinner=False)
def discover_all_files(base_dir: str) -> dict:
    root = Path(base_dir)
    found_docx  = rglob_many(root, DOC_EXTS)
    found_tab   = rglob_many(root, TABULAR_EXTS)
    found_pdf   = rglob_many(root, PDF_EXTS)
    return {"docx": sorted(found_docx), "tabular": sorted(found_tab), "pdf": sorted(found_pdf)}

FOUND = discover_all_files(str(BASE_DIR))

# =========================
# NEW: COLLECT & CHUNK TEXTS FOR RETRIEVAL
# =========================
def chunk_text(text: str, size: int = 900, overlap: int = 150) -> List[str]:
    text = (text or "").strip()
    if not text:
        return []
    chunks, i, n = [], 0, len(text)
    while i < n:
        j = min(i + size, n)
        chunks.append(text[i:j])
        i = j - overlap if j - overlap > i else j
    return [c for c in chunks if c.strip()]

@st.cache_data(show_spinner=False)
def collect_chunks(docx_files: List[Path], tabular_files: List[Path], pdf_files: List[Path]) -> List[Dict]:
    rows = []

    # DOCX
    for p in docx_files:
        try:
            txt = extract_text_from_docx(str(p))
            for c in chunk_text(txt):
                rows.append({"source": p.name, "kind": "DOCX", "text": c})
        except Exception:
            pass

    # TABULAR
    for p in tabular_files:
        try:
            if p.suffix.lower() == ".csv":
                txt = load_csv_as_text(str(p))
            else:
                txt = load_excel_as_text(str(p))
            for c in chunk_text(txt):
                rows.append({"source": p.name, "kind": "TABLE", "text": c})
        except Exception:
            pass

    # PDF
    for p in pdf_files:
        try:
            txt = extract_text_from_pdf(str(p))
            for c in chunk_text(txt):
                rows.append({"source": p.name, "kind": "PDF", "text": c})
        except Exception:
            pass

    return rows

CHUNKS = collect_chunks(FOUND["docx"], FOUND["tabular"], FOUND["pdf"])

# =========================
# NEW: BUILD TF-IDF INDEX (CHAR N-GRAM ‚Üí ‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
# =========================
@st.cache_resource(show_spinner=False)
def build_index(chunks: List[Dict]):
    texts = [r["text"] for r in chunks] or ["dummy"]
    vect = TfidfVectorizer(analyzer="char", ngram_range=(3, 5))
    X = vect.fit_transform(texts)
    return vect, X

VECT, X = build_index(CHUNKS)

def retrieve_context(query: str, top_k: int = 8, max_chars: int = 6000) -> Tuple[str, List[Tuple[int, float]]]:
    if not query.strip() or X.shape[0] == 0:
        return "", []
    qv = VECT.transform([query])
    sims = (X @ qv.T).toarray().ravel()  # cosine sim for tfidf-normalized
    idx = np.argsort(-sims)[:top_k]
    picked = [(int(i), float(sims[i])) for i in idx if sims[i] > 0]
    parts, total = [], 0
    for i, _ in picked:
        seg = CHUNKS[i]
        block = f"[{seg['kind']}] {seg['source']}\n{seg['text']}\n"
        if total + len(block) > max_chars:
            break
        parts.append(block); total += len(block)
    return "\n".join(parts), picked

# =========================
# BUILD REFERENCE STATUS (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sidebar ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
# =========================
# ‡∏ô‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡πà‡∏≤‡∏ß ‡πÜ ‡∏à‡∏≤‡∏Å‡∏ú‡∏• collect_chunks
LOAD_STATUS = {
    "docx": {},
    "tabular": {},
    "pdf": {},
}
for p in FOUND["docx"]:
    LOAD_STATUS["docx"][p.name] = "‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à" if any(r["source"] == p.name for r in CHUNKS) else "‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ"
for p in FOUND["tabular"]:
    LOAD_STATUS["tabular"][p.name] = "‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à" if any(r["source"] == p.name for r in CHUNKS) else "‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ"
for p in FOUND["pdf"]:
    LOAD_STATUS["pdf"][p.name] = "‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à" if any(r["source"] == p.name for r in CHUNKS) else "‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ"

# =========================
# SESSION STATE (MESSAGES)
# =========================
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏î‡∏Ñ‡∏∞"}
    ]
if "previous_messages" not in st.session_state:
    st.session_state["previous_messages"] = []

# =========================
# SIDEBAR
# =========================
with st.sidebar:
    st.header("‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó")
    col1, col2 = st.columns(2)
    if col1.button("üßπ Clear History"):
        clear_history()
    if col2.button("üîÅ Restore"):
        restore_history()

    st.markdown("---")
    st.header("‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå")
    st.write(f"- DOCX: {len(FOUND['docx'])} ‡πÑ‡∏ü‡∏•‡πå")
    st.write(f"- ‡∏ï‡∏≤‡∏£‡∏≤‡∏á (CSV/XLSX/XLS): {len(FOUND['tabular'])} ‡πÑ‡∏ü‡∏•‡πå")
    st.write(f"- PDF: {len(FOUND['pdf'])} ‡πÑ‡∏ü‡∏•‡πå")

    with st.expander("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î DOCX"):
        for name, s in LOAD_STATUS["docx"].items():
            st.info(f"{name} : {s}")
    with st.expander("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏£‡∏≤‡∏á (CSV/XLSX/XLS)"):
        for name, s in LOAD_STATUS["tabular"].items():
            st.info(f"{name} : {s}")
    with st.expander("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î PDF"):
        for name, s in LOAD_STATUS["pdf"].items():
            st.info(f"{name} : {s}")

# =========================
# RENDER HISTORY
# =========================
assistant_avatar = str(AVATAR_PATH) if AVATAR_PATH.exists() else None
for msg in st.session_state["messages"]:
    if msg["role"] == "assistant":
        st.chat_message("assistant", avatar=assistant_avatar).write(msg["content"])
    else:
        st.chat_message(msg["role"]).write(msg["content"])

# =========================
# BUILD HISTORY FOR GEMINI (‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤ ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß)
# =========================
def build_history_for_gemini(messages, context_text: str):
    history = []
    if context_text:
        history.append({"role": "user", "parts": [{"text": "‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (‡∏ã‡πà‡∏≠‡∏ô‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ):\n" + context_text}]})
    for m in messages:
        role = "user" if m["role"] == "user" else "model"
        history.append({"role": role, "parts": [{"text": m["content"]}]})
    return history

# =========================
# STREAMING + RETRY + FALLBACK
# =========================
def _should_retry(e: Exception) -> bool:
    msg = str(e).lower()
    return any(k in msg for k in ["429", "quota", "rate", "exceed", "resource exhausted", "deadline exceeded"])

def stream_typing_with_retry(history_payload, prompt_text: str,
                             typing_delay: float = 0.004,
                             retries: int = 2,
                             backoff: float = 2.0) -> str:
    status = st.empty()
    placeholder = st.empty()
    status.write("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö...")

    def _stream_from_model(model_name: str) -> str:
        nonlocal status, placeholder
        session = make_model(model_name).start_chat(history=history_payload)
        full_text, first_char_written = "", False
        for chunk in session.send_message(prompt_text, stream=True):
            text = getattr(chunk, "text", "") or ""
            for ch in text:
                if not first_char_written:
                    status.empty()
                    first_char_written = True
                full_text += ch
                placeholder.markdown(full_text)
                time.sleep(typing_delay)
        return full_text

    last_err = None
    for attempt in range(retries):
        try:
            return _stream_from_model(PRIMARY_MODEL_NAME)
        except Exception as e:
            last_err = e
            if _should_retry(e) and attempt < retries - 1:
                time.sleep(backoff); backoff *= 2
            else:
                break

    try:
        placeholder.markdown("**‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ñ‡πà‡∏∞...**")
        return _stream_from_model(FALLBACK_MODEL_NAME)
    except Exception:
        status.empty()
        placeholder.markdown("‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡πà‡∏∞")
        return ""

# =========================
# CHAT INPUT & RESPONSE
# =========================
prompt = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...")
if prompt:
    # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    st.session_state["messages"].append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # 1) ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏î‡∏±‡∏ä‡∏ô‡∏µ
    context_text, hits = retrieve_context(prompt, top_k=8, max_chars=6000)

    # 2) ‡∏™‡∏£‡πâ‡∏≤‡∏á history ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° '‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á' (‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô UI)
    history_payload = build_history_for_gemini(st.session_state["messages"][:-1], context_text)

    # 3) ‡∏™‡πà‡∏á‡∏ñ‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏°‡∏µ retry + fallback)
    with st.chat_message("assistant", avatar=assistant_avatar):
        reply = stream_typing_with_retry(history_payload, prompt_text=prompt, typing_delay=0.004)
        
    st.session_state["messages"].append({"role": "assistant", "content": (reply or "") + random.choice(followups)})
