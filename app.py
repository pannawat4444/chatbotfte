# app.py
import os
import time
import random
import pandas as pd
import streamlit as st
import docx
from PyPDF2 import PdfReader
from pathlib import Path

import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

from prompt import PROMPT_FTE  # р╕Хр╣Йр╕нр╕Зр╕бр╕╡р╣Др╕Яр╕ер╣М prompt.py р╕Чр╕╡р╣Ир╕Ыр╕гр╕░р╕Бр╕▓р╕и PROMPT_FTE

# =========================
# PATHS (GitHub/Streamlit Cloud friendly)
# =========================
BASE_DIR = Path(__file__).resolve().parent

# р╣Др╕Яр╕ер╣Мр╕нр╕вр╕╣р╣Ир╕Чр╕╡р╣Ир╕гр╕▓р╕Бр╕гр╕╡р╣Вр╕Ы (р╕Хр╕▓р╕бр╕ар╕▓р╕Ю GitHub р╕Чр╕╡р╣Ир╣Гр╕лр╣Йр╕бр╕▓)
EXCEL_CANDIDATES = [
    BASE_DIR / "FTE-DATASET.xlsx",
    BASE_DIR / "workaw_data.xlsx",   # р╣Ар╕Ьр╕╖р╣Ир╕нр╣Гр╕Кр╣Йр╣Др╕Яр╕ер╣Мр╕Щр╕╡р╣Йр╕Фр╣Йр╕зр╕в
]
DOCX_CANDIDATES  = [BASE_DIR / "Data Set No Question docx.docx"]
PDF_CANDIDATES   = [BASE_DIR / "Data Set No Question pdf.pdf"]

AVATAR_CANDIDATES = [BASE_DIR / "assets" / "green-bot.png"]  # р╕Цр╣Йр╕▓р╣Др╕бр╣Ир╕бр╕╡р╕Ир╕░р╣Др╕бр╣Ир╣Гр╕Кр╣Й

def pick_first_existing(paths):
    for p in paths:
        if p.exists():
            return p
    return None

EXCEL_PATH  = pick_first_existing(EXCEL_CANDIDATES)
DOCX_PATH   = pick_first_existing(DOCX_CANDIDATES)
PDF_PATH    = pick_first_existing(PDF_CANDIDATES)
AVATAR_PATH = pick_first_existing(AVATAR_CANDIDATES)

PAGE_ICON = str(AVATAR_PATH) if AVATAR_PATH else None

# =========================
# Page config & Header
# =========================
st.set_page_config(page_title="FTE Chatbot тАв KMUTNB", page_icon=PAGE_ICON, layout="centered")
st.title("ЁЯТм Welcome to Faculty of Technical Education, KMUTNB")

# =========================
# API Key & Model config
# =========================
api_key = st.secrets.get("GEMINI_APIKEY")
if not api_key:
    st.error("р╣Др╕бр╣Ир╕Юр╕Ъ GEMINI_APIKEY р╣Гр╕Щр╣Др╕Яр╕ер╣М .streamlit/secrets.toml р╣Вр╕Ыр╕гр╕Фр╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕Бр╕▓р╕гр╕Хр╕▒р╣Йр╕Зр╕Др╣Ир╕▓.")
    st.stop()

genai.configure(api_key=api_key)

generation_config = {
    "temperature": 0.1,
    "top_p": 0.95,
    "top_k": 64,
    "max_output_tokens": 1024,
    "response_mime_type": "text/plain",
}
SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}
MODEL_NAME = "gemini-2.0-flash"

model = genai.GenerativeModel(
    model_name=MODEL_NAME,
    safety_settings=SAFETY_SETTINGS,
    generation_config=generation_config,
    system_instruction=PROMPT_FTE,
)

# =========================
# Chat history utils
# =========================
def clear_history():
    st.session_state["previous_messages"] = st.session_state.get("messages", []).copy()
    st.session_state["messages"] = [{"role": "assistant", "content": "р╕Ыр╕гр╕░р╕зр╕▒р╕Хр╕┤р╕Бр╕▓р╕гр╣Ар╣Ар╕Кр╕Чр╕Вр╕нр╕Зр╕Чр╣Ир╕▓р╕Щ"}]
    st.rerun()

def restore_history():
    if st.session_state.get("previous_messages"):
        st.session_state["messages"] = st.session_state["previous_messages"].copy()
    else:
        st.warning("р╣Др╕бр╣Ир╕Юр╕Ър╕Ыр╕гр╕░р╕зр╕▒р╕Хр╕┤р╕Чр╕╡р╣Ир╕кр╕▓р╕бр╕▓р╕гр╕Цр╣Ар╕гр╕╡р╕вр╕Бр╕Др╕╖р╕Щр╣Др╕Фр╣Й")
    st.rerun()

# =========================
# File readers (path-based) + cache
# =========================
@st.cache_data(show_spinner=False)
def extract_text_from_docx(docx_path: str) -> str:
    try:
        d = docx.Document(docx_path)
        return "\n".join([p.text for p in d.paragraphs])
    except Exception as e:
        st.error(f"Error reading Word file '{docx_path}': {e}")
        return ""

@st.cache_data(show_spinner=False)
def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        reader = PdfReader(pdf_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() or ""
        return text
    except Exception as e:
        st.error(f"Error reading PDF file '{pdf_path}': {e}")
        return ""

@st.cache_data(show_spinner=False)
def load_excel_as_text(excel_path: str, max_rows: int = 80) -> str:
    try:
        if not os.path.exists(excel_path):
            return ""
        df = pd.read_excel(excel_path)
        # р╕ер╕Ф payload
        if df.shape[1] > 8:
            df = df.iloc[:, :8]
        if len(df) > max_rows:
            df = df.head(max_rows)
        return df.to_csv(index=False)
    except Exception as e:
        st.error(f"Error reading Excel file '{excel_path}': {e}")
        return ""

# =========================
# Build reference corpus + Sidebar status
# =========================
def build_reference_corpus_with_sidebar_status(
    excel_file: Path | None = EXCEL_PATH,
    word_file: Path | None = DOCX_PATH,
    pdf_file:  Path | None = PDF_PATH,
    max_chars: int = 45_000,
) -> str:
    def _name(p: Path | None) -> str:
        return p.name if isinstance(p, Path) else "N/A"

    st.session_state["excel_file_name"] = _name(excel_file)
    st.session_state["word_file_name"]  = _name(word_file)
    st.session_state["pdf_file_name"]   = _name(pdf_file)

    st.session_state.setdefault("excel_status", "р╕вр╕▒р╕Зр╣Др╕бр╣Ир╣Др╕Фр╣Йр╣Вр╕лр╕ер╕Ф")
    st.session_state.setdefault("word_status",  "р╕вр╕▒р╕Зр╣Др╕бр╣Ир╣Др╕Фр╣Йр╣Вр╕лр╕ер╕Ф")
    st.session_state.setdefault("pdf_status",   "р╕вр╕▒р╕Зр╣Др╕бр╣Ир╣Др╕Фр╣Йр╣Вр╕лр╕ер╕Ф")

    parts = []

    # Excel
    if isinstance(excel_file, Path) and excel_file.exists():
        try:
            st.session_state["excel_status"] = "р╕Бр╕│р╕ер╕▒р╕Зр╣Вр╕лр╕ер╕Ф..."
            txt = load_excel_as_text(str(excel_file))
            if txt:
                parts.append("р╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Ир╕▓р╕Бр╣Др╕Яр╕ер╣М Excel (CSV):\n" + txt)
            st.session_state["excel_status"] = "р╣Вр╕лр╕ер╕Фр╕кр╕│р╣Ар╕гр╣Зр╕И"
        except Exception as e:
            st.session_state["excel_status"] = f"р╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Ф: {e}"
    else:
        st.session_state["excel_status"] = "р╣Др╕бр╣Ир╕Юр╕Ър╣Др╕Яр╕ер╣М"

    # Word
    if isinstance(word_file, Path) and word_file.exists():
        try:
            st.session_state["word_status"] = "р╕Бр╕│р╕ер╕▒р╕Зр╣Вр╕лр╕ер╕Ф..."
            txt = extract_text_from_docx(str(word_file))
            if txt:
                parts.append("р╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Ир╕▓р╕Бр╣Др╕Яр╕ер╣М Word:\n" + txt)
            st.session_state["word_status"] = "р╣Вр╕лр╕ер╕Фр╕кр╕│р╣Ар╕гр╣Зр╕И"
        except Exception as e:
            st.session_state["word_status"] = f"р╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Ф: {e}"
    else:
        st.session_state["word_status"] = "р╣Др╕бр╣Ир╕Юр╕Ър╣Др╕Яр╕ер╣М"

    # PDF
    if isinstance(pdf_file, Path) and pdf_file.exists():
        try:
            st.session_state["pdf_status"] = "р╕Бр╕│р╕ер╕▒р╕Зр╣Вр╕лр╕ер╕Ф..."
            txt = extract_text_from_pdf(str(pdf_file))
            if txt:
                parts.append("р╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Ир╕▓р╕Бр╣Др╕Яр╕ер╣М PDF:\n" + txt)
            st.session_state["pdf_status"] = "р╣Вр╕лр╕ер╕Фр╕кр╕│р╣Ар╕гр╣Зр╕И"
        except Exception as e:
            st.session_state["pdf_status"] = f"р╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Ф: {e}"
    else:
        st.session_state["pdf_status"] = "р╣Др╕бр╣Ир╕Юр╕Ър╣Др╕Яр╕ер╣М"

    blob = "\n\n".join(parts).strip()
    if len(blob) > max_chars:
        blob = blob[:max_chars] + "\n\n[TRUNCATED]"
    return blob

# р╣Вр╕лр╕ер╕Фр╕Др╕нр╕гр╣Мр╕Ыр╕▒р╕к (р╕Бр╣Ир╕нр╕Щ Render Sidebar)
REFERENCE_BLOB = build_reference_corpus_with_sidebar_status()

# =========================
# Session State (messages)
# =========================
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "р╕Др╕╕р╕Ур╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕гр╕кр╕нр╕Ър╕Цр╕▓р╕бр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Вр╕нр╕Зр╕Др╕Ур╕░р╕Др╕гр╕╕р╕ир╕▓р╕кр╕Хр╕гр╣Мр╕нр╕╕р╕Хр╕кр╕▓р╕лр╕Бр╕гр╕гр╕бр╣Ар╕гр╕╖р╣Ир╕нр╕Зр╣Гр╕Фр╕Др╕░"}
    ]
if "previous_messages" not in st.session_state:
    st.session_state["previous_messages"] = []

# =========================
# Sidebar (Clear/Restore + р╕кр╕Цр╕▓р╕Щр╕░р╣Др╕Яр╕ер╣М)
# =========================
with st.sidebar:
    st.header("р╕Бр╕▓р╕гр╕Ир╕▒р╕Фр╕Бр╕▓р╕гр╣Бр╕Кр╕Ч")
    if st.button("Clear History"):
        clear_history()
    if st.button("Restore Last History"):
        restore_history()

    st.markdown("---")
    st.header("р╕кр╕Цр╕▓р╕Щр╕░р╕Бр╕▓р╕гр╣Вр╕лр╕ер╕Фр╣Др╕Яр╕ер╣Мр╕Вр╣Йр╕нр╕бр╕╣р╕е")
    st.info(f"Excel ({st.session_state.get('excel_file_name', 'N/A')}): {st.session_state.get('excel_status', 'р╕вр╕▒р╕Зр╣Др╕бр╣Ир╣Др╕Фр╣Йр╣Вр╕лр╕ер╕Ф')}")
    st.info(f"Word ({st.session_state.get('word_file_name', 'N/A')}): {st.session_state.get('word_status', 'р╕вр╕▒р╕Зр╣Др╕бр╣Ир╣Др╕Фр╣Йр╣Вр╕лр╕ер╕Ф')}")
    st.info(f"PDF ({st.session_state.get('pdf_file_name', 'N/A')}): {st.session_state.get('pdf_status', 'р╕вр╕▒р╕Зр╣Др╕бр╣Ир╣Др╕Фр╣Йр╣Вр╕лр╕ер╕Ф')}")

# =========================
# Render History
# =========================
assistant_avatar = str(AVATAR_PATH) if AVATAR_PATH else None

for msg in st.session_state["messages"]:
    if msg["role"] == "assistant":
        st.chat_message("assistant", avatar=assistant_avatar).write(msg["content"])
    else:
        st.chat_message(msg["role"]).write(msg["content"])

# =========================
# Build history for Gemini
# =========================
def build_history_for_gemini(messages):
    history = []
    if REFERENCE_BLOB:
        history.append({"role": "user", "parts": [{"text": "р╕Вр╣Йр╕нр╕бр╕╣р╕ер╕нр╣Йр╕▓р╕Зр╕нр╕┤р╕З:\n" + REFERENCE_BLOB}]})
    for m in messages:
        role = "user" if m["role"] == "user" else "model"
        history.append({"role": role, "parts": [{"text": m["content"]}]})
    return history

# =========================
# Typing-effect streaming
# =========================
def stream_typing_response(chat_session, prompt_text: str, typing_delay: float = 0.004) -> str:
    status = st.empty()      # р╣Бр╕кр╕Фр╕Зр╕кр╕Цр╕▓р╕Щр╕░р╕Кр╕▒р╣Ир╕зр╕Др╕гр╕▓р╕з
    placeholder = st.empty() # р╕Чр╕╡р╣Ир╣Гр╕кр╣Ир╕Вр╣Йр╕нр╕Др╕зр╕▓р╕бр╕Юр╕┤р╕бр╕Юр╣Мр╣Др╕лр╕е р╣Ж
    status.write("р╕Бр╕│р╕ер╕▒р╕Зр╕Др╣Йр╕Щр╕лр╕▓р╕Др╕│р╕Хр╕нр╕Ъ...")

    full_text = ""
    first_char_written = False
    try:
        for chunk in chat_session.send_message(prompt_text, stream=True):
            text = getattr(chunk, "text", "") or ""
            for ch in text:
                if not first_char_written:
                    status.empty()
                    first_char_written = True
                full_text += ch
                placeholder.markdown(full_text)
                time.sleep(typing_delay)
        status.empty()
    except Exception as e:
        status.empty()
        placeholder.markdown(f"р╣Ар╕Бр╕┤р╕Фр╕Вр╣Йр╕нр╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Фр╕гр╕░р╕лр╕зр╣Ир╕▓р╕Зр╕кр╕Хр╕гр╕╡р╕бр╕Др╕│р╕Хр╕нр╕Ъ: {e}")
    return full_text

# =========================
# Chat input & response
# =========================
prompt = st.chat_input("р╕Юр╕┤р╕бр╕Юр╣Мр╕Др╕│р╕Цр╕▓р╕бр╕Вр╕нр╕Зр╕Др╕╕р╕Ур╕Чр╕╡р╣Ир╕Щр╕╡р╣И...")
if prompt:
    # р╕Ьр╕╣р╣Йр╣Гр╕Кр╣Й
    st.session_state["messages"].append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # р╕Ыр╕гр╕░р╕зр╕▒р╕Хр╕┤ + р╣Ар╕гр╕┤р╣Ир╕бр╣Бр╕Кр╕Чр╕Бр╕▒р╕Ър╣Вр╕бр╣Ар╕Фр╕е
    history_payload = build_history_for_gemini(st.session_state["messages"][:-1])
    chat_session = model.start_chat(history=history_payload)

    # р╕Ьр╕╣р╣Йр╕Кр╣Ир╕зр╕в
    with st.chat_message("assistant", avatar=assistant_avatar):
        normalized = prompt.strip().lower()

        if normalized == "history":
            history_text = "\n".join(
                [f"{m['role'].capitalize()}: {m['content']}" for m in st.session_state["messages"]]
            )
            tmp_session = genai.GenerativeModel(
                model_name=MODEL_NAME,
                safety_settings=SAFETY_SETTINGS,
                generation_config=generation_config,
                system_instruction=PROMPT_FTE,
            ).start_chat(history=[])
            reply = stream_typing_response(
                chat_session=tmp_session,
                prompt_text=f"р╕кр╕гр╕╕р╕Ыр╕Ыр╕гр╕░р╕зр╕▒р╕Хр╕┤р╕Бр╕▓р╕гр╕кр╕Щр╕Чр╕Щр╕▓р╕Щр╕╡р╣Йр╣Гр╕лр╣Йр╕нр╣Ир╕▓р╕Щр╕Зр╣Ир╕▓р╕в р╕Бр╕гр╕░р╕Кр╕▒р╕Ъ р╣Бр╕ер╕░р╣Ар╕Ыр╣Зр╕Щр╕Бр╕▒р╕Щр╣Ар╕нр╕З:\n\n{history_text}",
                typing_delay=0.003
            )
        else:
            reply = stream_typing_response(chat_session, prompt_text=prompt, typing_delay=0.004)

    # р╕Ыр╕┤р╕Фр╕Чр╣Йр╕▓р╕вр╕Чр╕╕р╕Бр╕Др╕│р╕Хр╕нр╕Ър╕Фр╣Йр╕зр╕вр╕Ыр╕гр╕░р╣Вр╕вр╕Др╕кр╕╕р╕ар╕▓р╕Юр╣Бр╕Ър╕Ър╕кр╕╕р╣Ир╕б (р╣Др╕бр╣Ир╕бр╕╡р╕нр╕╡р╣Вр╕бр╕Ир╕┤)
    followups = [
        "\n\nр╕бр╕╡р╕нр╕░р╣Др╕гр╣Ар╕Юр╕┤р╣Ир╕бр╣Ар╕Хр╕┤р╕бр╕Чр╕╡р╣Ир╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕гр╣Гр╕лр╣Йр╕Йр╕▒р╕Щр╕Кр╣Ир╕зр╕вр╕нр╕╡р╕Бр╣Др╕лр╕бр╕Др╕░",
        "\n\nр╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕гр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕кр╣Ир╕зр╕Щр╣Др╕лр╕Щр╣Ар╕Юр╕┤р╣Ир╕бр╣Ар╕Хр╕┤р╕бр╕нр╕╡р╕Бр╣Др╕лр╕бр╕Др╕░",
        "\n\nр╕нр╕вр╕▓р╕Бр╣Гр╕лр╣Йр╕Кр╣Ир╕зр╕вр╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕гр╕▓р╕вр╕ер╕░р╣Ар╕нр╕╡р╕вр╕Фр╕нр╕╖р╣Ир╕Щр╣Ар╕Юр╕┤р╣Ир╕бр╣Ар╕Хр╕┤р╕бр╣Др╕лр╕бр╕Др╕░",
        "\n\nр╕бр╕╡р╕лр╕▒р╕зр╕Вр╣Йр╕нр╕нр╕╖р╣Ир╕Щр╕Вр╕нр╕Зр╕Др╕Ур╕░р╕Др╕гр╕╕р╕ир╕▓р╕кр╕Хр╕гр╣Мр╕нр╕╕р╕Хр╕кр╕▓р╕лр╕Бр╕гр╕гр╕бр╕Чр╕╡р╣Ир╕нр╕вр╕▓р╕Бр╕Чр╕гр╕▓р╕Ър╕нр╕╡р╕Бр╣Др╕лр╕бр╕Др╕░",
        "\n\nр╕лр╕▓р╕Бр╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕гр╕Вр╣Йр╕нр╕бр╕╣р╕ер╣Ар╕Кр╕┤р╕Зр╕ер╕╢р╕Б р╕гр╕░р╕Ър╕╕р╕гр╕лр╕▒р╕кр╕зр╕┤р╕Кр╕▓/р╕Кр╕╖р╣Ир╕нр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕гр╣Ар╕Юр╕┤р╣Ир╕бр╣Ар╕Хр╕┤р╕бр╣Др╕Фр╣Йр╣Ар╕ер╕вр╕Щр╕░р╕Др╕░",
    ]
    reply_with_followup = reply + random.choice(followups)

    # р╣Ар╕Бр╣Зр╕Ър╕ер╕Зр╕Ыр╕гр╕░р╕зр╕▒р╕Хр╕┤
    st.session_state["messages"].append({"role": "assistant", "content": reply_with_followup})
