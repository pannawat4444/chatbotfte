# app.py
import os
import time
import random
from pathlib import Path
from typing import Iterable, List, Dict, Tuple
import pandas as pd
import streamlit as st
import docx
from PyPDF2 import PdfReader

import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

from prompt import PROMPT_FTE  # à¸•à¹‰à¸­à¸‡à¸¡à¸µà¹„à¸Ÿà¸¥à¹Œ prompt.py à¸—à¸µà¹ˆà¸›à¸£à¸°à¸à¸²à¸¨ PROMPT_FTE

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# =========================
# PATHS & DISCOVERY
# =========================
BASE_DIR = Path(__file__).resolve().parent

DOC_EXTS     = {".docx"}
TABULAR_EXTS = {".csv", ".xlsx", ".xls"}
PDF_EXTS     = {".pdf"}

AVATAR_PATH = BASE_DIR / "assets" / "green-bot.png"
PAGE_ICON = str(AVATAR_PATH) if AVATAR_PATH.exists() else None

# =========================
# PAGE CONFIG & HEADER
# =========================
st.set_page_config(page_title="FTE Chatbot â€¢ KMUTNB", page_icon=PAGE_ICON, layout="centered")
st.title("ðŸ—¨ï¸ Computer Education & Civil Engineering and Education Chatbot â€¢ FTE of KMUTNB")

# =========================
# API KEY & MODEL CONFIG
# =========================
api_key = st.secrets.get("GEMINI_APIKEY")
if not api_key:
    st.error("à¹„à¸¡à¹ˆà¸žà¸š GEMINI_APIKEY à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ .streamlit/secrets.toml à¹‚à¸›à¸£à¸”à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸².")
    st.stop()

genai.configure(api_key=api_key)

generation_config = {
    "temperature": 0.5,
    "top_p": 0.95,
    "top_k": 64,
    "max_output_tokens": 1024,
    "response_mime_type": "text/plain",
}
SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}
PRIMARY_MODEL_NAME = "gemini-2.5-flash"
FALLBACK_MODEL_NAME = "gemini-2.0-flash"

def make_model(name: str) -> genai.GenerativeModel:
    return genai.GenerativeModel(
        model_name=name,
        safety_settings=SAFETY_SETTINGS,
        generation_config=generation_config,
        system_instruction=PROMPT_FTE,
    )

# =========================
# CHAT HISTORY UTILS
# =========================
def clear_history():
    st.session_state["previous_messages"] = st.session_state.get("messages", []).copy()
    st.session_state["messages"] = [{"role": "assistant", "content": "à¸›à¸£à¸°à¸§à¸±à¸•à¸´à¸à¸²à¸£à¹€à¹€à¸Šà¸—à¸‚à¸­à¸‡à¸—à¹ˆà¸²à¸™"}]
    st.rerun()

def restore_history():
    if st.session_state.get("previous_messages"):
        st.session_state["messages"] = st.session_state["previous_messages"].copy()
    else:
        st.warning("à¹„à¸¡à¹ˆà¸žà¸šà¸›à¸£à¸°à¸§à¸±à¸•à¸´à¸—à¸µà¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸£à¸µà¸¢à¸à¸„à¸·à¸™à¹„à¸”à¹‰")
    st.rerun()

# =========================
# FILE READERS (CACHED) PDF & Docx
# =========================
@st.cache_data(show_spinner=False)
def extract_text_from_docx(docx_path: str) -> str:
    try:
        d = docx.Document(docx_path)
        return "\n".join([p.text for p in d.paragraphs if p.text.strip()])
    except Exception as e:
        st.error(f"Error reading Word file '{docx_path}': {e}")
        return ""

@st.cache_data(show_spinner=False)
def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        reader = PdfReader(pdf_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() or ""
        return text
    except Exception as e:
        st.error(f"Error reading PDF file '{pdf_path}': {e}")
        return ""

@st.cache_data(show_spinner=False)
def load_excel_as_text(excel_path: str, max_rows: int = 160, max_cols: int = 12) -> str:
    try:
        if not os.path.exists(excel_path):
            return ""
        df = pd.read_excel(excel_path)
        if df.shape[1] > max_cols:
            df = df.iloc[:, :max_cols]
        if len(df) > max_rows:
            df = df.head(max_rows)
        return df.to_csv(index=False)
    except Exception as e:
        st.error(f"Error reading Excel file '{excel_path}': {e}")
        return ""

@st.cache_data(show_spinner=False)
def load_csv_as_text(csv_path: str, max_rows: int = 200, max_cols: int = 12) -> str:
    try:
        if not os.path.exists(csv_path):
            return ""
        df = pd.read_csv(csv_path, engine="python", encoding_errors="ignore")
        if df.shape[1] > max_cols:
            df = df.iloc[:, :max_cols]
        if len(df) > max_rows:
            df = df.head(max_rows)
        return df.to_csv(index=False)
    except Exception as e:
        st.error(f"Error reading CSV file '{csv_path}': {e}")
        return "" 

# =========================
# DISCOVER FILES (RECURSIVE)
# =========================
def rglob_many(root: Path, exts: Iterable[str]) -> list[Path]:
    exts_low = {e.lower() for e in exts}
    # à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚ .name.startswith("~$") à¹€à¸žà¸·à¹ˆà¸­à¸¥à¸°à¹€à¸§à¹‰à¸™à¹„à¸Ÿà¸¥à¹Œà¸Šà¸±à¹ˆà¸§à¸„à¸£à¸²à¸§
    return [p for p in root.rglob("*") if p.is_file() and p.suffix.lower() in exts_low and not p.name.startswith("~$")]

@st.cache_data(show_spinner=False)
def discover_all_files(base_dir: str) -> dict:
    root = Path(base_dir)
    found_docx  = rglob_many(root, DOC_EXTS)
    found_tab   = rglob_many(root, TABULAR_EXTS)
    found_pdf   = rglob_many(root, PDF_EXTS)

    dataset_file = root / "workaw" / "Data à¸„à¸³à¸•à¸­à¸š  à¸„à¸£à¸¸à¸¥à¹ˆà¸²à¸ªà¸¸à¸”.docx"
    if dataset_file.exists():
        found_docx.append(dataset_file)
    return {"docx": sorted(found_docx), "tabular": sorted(found_tab), "pdf": sorted(found_pdf)}

FOUND = discover_all_files(str(BASE_DIR))

# =========================
# NEW: COLLECT & CHUNK TEXTS FOR RETRIEVAL
# =========================
def chunk_text(text: str, size: int = 900, overlap: int = 150) -> List[str]:
    text = (text or "").strip()
    if not text:
        return []
    chunks, i, n = [], 0, len(text)
    while i < n:
        j = min(i + size, n)
        chunks.append(text[i:j])
        i = j - overlap if j - overlap > i else j
    return [c for c in chunks if c.strip()]

@st.cache_data(show_spinner=False)
def collect_chunks(docx_files: List[Path], tabular_files: List[Path], pdf_files: List[Path]) -> List[Dict]:
    rows = []

    # DOCX
    for p in docx_files:
        try:
            txt = extract_text_from_docx(str(p))
            if txt:  # à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚à¹€à¸žà¸·à¹ˆà¸­à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
                for c in chunk_text(txt):
                    rows.append({"source": p.name, "kind": "DOCX", "text": c})
        except Exception:
            pass

    # TABULAR
    for p in tabular_files:
        try:
            if p.suffix.lower() == ".csv":
                txt = load_csv_as_text(str(p))
            else:
                txt = load_excel_as_text(str(p))
            if txt:
                for c in chunk_text(txt):
                    rows.append({"source": p.name, "kind": "TABLE", "text": c})
        except Exception:
            pass

    # PDF
    for p in pdf_files:
        try:
            txt = extract_text_from_pdf(str(p))
            if txt:
                for c in chunk_text(txt):
                    rows.append({"source": p.name, "kind": "PDF", "text": c})
        except Exception:
            pass

    return rows

CHUNKS = collect_chunks(FOUND["docx"], FOUND["tabular"], FOUND["pdf"])

# =========================
# NEW: BUILD TF-IDF INDEX (CHAR N-GRAM â†’ à¸”à¸µà¸ªà¸³à¸«à¸£à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢)
# =========================
@st.cache_resource(show_spinner=False)
def build_index(chunks: List[Dict]):
    texts = [r["text"] for r in chunks] or ["dummy"]
    vect = TfidfVectorizer(analyzer="char", ngram_range=(3, 5))
    X = vect.fit_transform(texts)
    return vect, X

VECT, X = build_index(CHUNKS)

def retrieve_context(query: str, top_k: int = 8, max_chars: int = 6000) -> Tuple[str, List[Tuple[int, float]]]:
    if not query.strip() or X.shape[0] == 0:
        return "", []
    qv = VECT.transform([query])
    sims = (X @ qv.T).toarray().ravel()  # cosine sim for tfidf-normalized
    idx = np.argsort(-sims)[:top_k]
    picked = [(int(i), float(sims[i])) for i in idx if sims[i] > 0]
    parts, total = [], 0
    for i, _ in picked:
        seg = CHUNKS[i]
        block = f"[{seg['kind']}] {seg['source']}\n{seg['text']}\n"
        if total + len(block) > max_chars:
            break
        parts.append(block); total += len(block)
    return "\n".join(parts), picked

# =========================
# BUILD REFERENCE STATUS (à¸ªà¸³à¸«à¸£à¸±à¸š Sidebar à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™)
# =========================
LOAD_STATUS = {
    "docx": {},
    "tabular": {},
    "pdf": {},
}
for p in FOUND["docx"]:
    LOAD_STATUS["docx"][p.name] = "à¹‚à¸«à¸¥à¸”à¸ªà¸³à¹€à¸£à¹‡à¸ˆ" if any(r["source"] == p.name for r in CHUNKS) else "à¹„à¸Ÿà¸¥à¹Œà¸§à¹ˆà¸²à¸‡à¸«à¸£à¸·à¸­à¸­à¹ˆà¸²à¸™à¹„à¸¡à¹ˆà¹„à¸”à¹‰"
for p in FOUND["tabular"]:
    LOAD_STATUS["tabular"][p.name] = "à¹‚à¸«à¸¥à¸”à¸ªà¸³à¹€à¸£à¹‡à¸ˆ" if any(r["source"] == p.name for r in CHUNKS) else "à¹„à¸Ÿà¸¥à¹Œà¸§à¹ˆà¸²à¸‡à¸«à¸£à¸·à¸­à¸­à¹ˆà¸²à¸™à¹„à¸¡à¹ˆà¹„à¸”à¹‰"
for p in FOUND["pdf"]:
    LOAD_STATUS["pdf"][p.name] = "à¹‚à¸«à¸¥à¸”à¸ªà¸³à¹€à¸£à¹‡à¸ˆ" if any(r["source"] == p.name for r in CHUNKS) else "à¹„à¸Ÿà¸¥à¹Œà¸§à¹ˆà¸²à¸‡à¸«à¸£à¸·à¸­à¸­à¹ˆà¸²à¸™à¹„à¸¡à¹ˆà¹„à¸”à¹‰"

# =========================
# SESSION STATE (MESSAGES)
# =========================
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "à¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸ªà¸­à¸šà¸–à¸²à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸£à¸·à¹ˆà¸­à¸‡à¹ƒà¸”à¸„à¸°"}
    ]
if "previous_messages" not in st.session_state:
    st.session_state["previous_messages"] = []

# =========================
# SIDEBAR
# =========================
with st.sidebar:
    st.header("à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£à¹à¸Šà¸—")
    col1, col2 = st.columns(2)
    if col1.button("ðŸ§¹ Clear History"):
        clear_history()
    if col2.button("ðŸ” Restore"):
        restore_history()


    st.markdown("---")
    #st.header("à¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¸žà¸šà¹ƒà¸™à¹‚à¸›à¸£à¹€à¸ˆà¹‡à¸à¸•à¹Œ")
    #st.write(f"- DOCX: {len(FOUND['docx'])} à¹„à¸Ÿà¸¥à¹Œ")
    #st.write(f"- à¸•à¸²à¸£à¸²à¸‡ (CSV/XLSX/XLS): {len(FOUND['tabular'])} à¹„à¸Ÿà¸¥à¹Œ")
    #st.write(f"- PDF: {len(FOUND['pdf'])} à¹„à¸Ÿà¸¥à¹Œ")

    #with st.expander("à¸ªà¸–à¸²à¸™à¸°à¸à¸²à¸£à¹‚à¸«à¸¥à¸” DOCX"):
      #  for name, s in LOAD_STATUS["docx"].items():
            #st.info(f"{name} : {s}")
    #with st.expander("à¸ªà¸–à¸²à¸™à¸°à¸à¸²à¸£à¹‚à¸«à¸¥à¸”à¸•à¸²à¸£à¸²à¸‡ (CSV/XLSX/XLS)"):
        #for name, s in LOAD_STATUS["tabular"].items():
           # st.info(f"{name} : {s}")
   # with st.expander("à¸ªà¸–à¸²à¸™à¸°à¸à¸²à¸£à¹‚à¸«à¸¥à¸” PDF"):
        #for name, s in LOAD_STATUS["pdf"].items():
           # st.info(f"{name} : {s}")

# =========================
# RENDER HISTORY
# =========================
assistant_avatar = str(AVATAR_PATH) if AVATAR_PATH.exists() else None
for msg in st.session_state["messages"]:
    if msg["role"] == "assistant":
        st.chat_message("assistant", avatar=assistant_avatar).write(msg["content"])
    else:
        st.chat_message(msg["role"]).write(msg["content"])

# =========================
# BUILD HISTORY FOR GEMINI (à¹ƒà¸Šà¹‰à¸šà¸£à¸´à¸šà¸—à¸—à¸µà¹ˆà¸”à¸¶à¸‡à¸¡à¸² à¹€à¸‰à¸žà¸²à¸°à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§)
# =========================
def build_history_for_gemini(messages, context_text: str):
    history = []
    if context_text:
        history.append({"role": "user", "parts": [{"text": "à¸šà¸£à¸´à¸šà¸—à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡ (à¸‹à¹ˆà¸­à¸™à¸ˆà¸²à¸à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰):\n" + context_text}]})
    for m in messages:
        role = "user" if m["role"] == "user" else "model"
        history.append({"role": role, "parts": [{"text": m["content"]}]})
    return history

# =========================
# STREAMING + RETRY + FALLBACK
# =========================
def _should_retry(e: Exception) -> bool:
    msg = str(e).lower()
    return any(k in msg for k in ["429", "quota", "rate", "exceed", "resource exhausted", "deadline exceeded"])

def stream_typing_with_retry(history_payload, prompt_text: str,
                             typing_delay: float = 0.004,
                             retries: int = 2,
                             backoff: float = 2.0) -> str:
    status = st.empty()
    placeholder = st.empty()
    status.write("à¸à¸³à¸¥à¸±à¸‡à¸„à¹‰à¸™à¸«à¸²à¸„à¸³à¸•à¸­à¸š...")

    def _stream_from_model(model_name: str) -> str:
        nonlocal status, placeholder
        session = make_model(model_name).start_chat(history=history_payload)
        full_text, first_char_written = "", False
        for chunk in session.send_message(prompt_text, stream=True):
            text = getattr(chunk, "text", "") or ""
            for ch in text:
                if not first_char_written:
                    status.empty()
                    first_char_written = True
                full_text += ch
                placeholder.markdown(full_text)
                time.sleep(typing_delay)
        return full_text

    last_err = None
    for attempt in range(retries):
        try:
            return _stream_from_model(PRIMARY_MODEL_NAME)
        except Exception as e:
            last_err = e
            if _should_retry(e) and attempt < retries - 1:
                time.sleep(backoff); backoff *= 2
            else:
                break

    try:
        placeholder.markdown("**à¸ªà¸¥à¸±à¸šà¹„à¸›à¹ƒà¸Šà¹‰à¹‚à¸¡à¹€à¸”à¸¥à¸ªà¸³à¸£à¸­à¸‡à¸Šà¸±à¹ˆà¸§à¸„à¸£à¸²à¸§à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹„à¸”à¹‰à¸„à¸³à¸•à¸­à¸šà¸„à¹ˆà¸°...**")
        return _stream_from_model(FALLBACK_MODEL_NAME)
    except Exception:
        status.empty()
        placeholder.markdown("à¸‚à¸­à¸­à¸ à¸±à¸¢à¸„à¹ˆà¸° à¸£à¸°à¸šà¸šà¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸šà¹„à¸”à¹‰à¹ƒà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆà¸ à¸²à¸¢à¸«à¸¥à¸±à¸‡à¸„à¹ˆà¸°")
        return ""

# =========================
# CHAT INPUT & RESPONSE
# =========================
prompt = st.chat_input("à¸žà¸´à¸¡à¸žà¹Œà¸„à¸³à¸–à¸²à¸¡à¸‚à¸­à¸‡à¸„à¸¸à¸“à¸—à¸µà¹ˆà¸™à¸µà¹ˆ...")
if prompt:
    # à¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰
    st.session_state["messages"].append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # 1) à¸”à¸¶à¸‡à¸šà¸£à¸´à¸šà¸—à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸ˆà¸²à¸à¸”à¸±à¸Šà¸™à¸µ
    context_text, hits = retrieve_context(prompt, top_k=8, max_chars=6000)

    # 2) à¸ªà¸£à¹‰à¸²à¸‡ history à¸—à¸µà¹ˆà¸£à¸§à¸¡ 'à¸šà¸£à¸´à¸šà¸—à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡' (à¸ˆà¸°à¹„à¸¡à¹ˆà¸–à¸¹à¸à¹à¸ªà¸”à¸‡à¹ƒà¸™ UI)
    history_payload = build_history_for_gemini(st.session_state["messages"][:-1], context_text)

    # 3) à¸ªà¹ˆà¸‡à¸–à¸²à¸¡à¹‚à¸¡à¹€à¸”à¸¥ (à¸¡à¸µ retry + fallback)
    with st.chat_message("assistant", avatar=assistant_avatar):
        reply = stream_typing_with_retry(history_payload, prompt_text=prompt, typing_delay=0.004)

    # à¸›à¸´à¸”à¸—à¹‰à¸²à¸¢à¸—à¸¸à¸à¸„à¸³à¸•à¸­à¸šà¸”à¹‰à¸§à¸¢à¸›à¸£à¸°à¹‚à¸¢à¸„à¸ªà¸¸à¸ à¸²à¸žà¹à¸šà¸šà¸ªà¸¸à¹ˆà¸¡ (à¹„à¸¡à¹ˆà¸¡à¸µà¸­à¸µà¹‚à¸¡à¸ˆà¸´)
    followups = [
        "\n\nà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¹ˆà¸§à¸™à¹„à¸«à¸™à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡à¸­à¸µà¸à¹„à¸«à¸¡à¸„à¸°"
    ]
    reply_with_followup = (reply or "") + random.choice(followups)
    
    st.session_state["messages"].append({"role": "assistant", "content": (reply or "") + random.choice(followups)})
